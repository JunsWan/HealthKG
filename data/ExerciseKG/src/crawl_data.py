import json
import time
import cloudscraper
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ====================================================
# å‚æ•°é…ç½®
# ====================================================
BASE_URL = "https://exrx.net"
BODY_PARTS = [
    "Neck", "Should", "Arm", "ForeArm",
    "Back", "Chest", "Waist", "Hips", "Thigh", "Calf"
]
OUTPUT_JSON = "../data/exrx_full_dataset.json"

# åˆå§‹åŒ– Scraperï¼ˆæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼‰
scraper = cloudscraper.create_scraper(
    browser={"browser": "chrome", "platform": "windows", "mobile": False}
)
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/125.0.0.0 Safari/537.36"
    ),
    "Referer": "https://www.google.com/",
    "Accept-Language": "en-US,en;q=0.9",
}

# ====================================================
# å·¥å…·å‡½æ•°
# ====================================================
def safe_get(url, retries=3, delay=3):
    """å®‰å…¨è¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    for i in range(retries):
        try:
            r = scraper.get(url, headers=HEADERS, timeout=25)
            if r.status_code == 200:
                return r.text
            print(f"âš ï¸ çŠ¶æ€ç  {r.status_code}ï¼Œé‡è¯•ä¸­ ({i+1}/{retries})...")
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸ {i+1}/{retries}: {e}")
        time.sleep(delay)
    return None


def normalize_exercise_url(href):
    """æ ‡å‡†åŒ– URL"""
    href = href.strip()
    if href.startswith("http"):
        return href
    if href.startswith("../../"):
        href = href.replace("../../", "")
    return urljoin(BASE_URL + "/", href)


# ====================================================
# åŠ¨ä½œè¯¦æƒ…é¡µè§£æ
# ====================================================
def parse_exercise_detail(url):
    """è§£æå•ä¸ªåŠ¨ä½œè¯¦æƒ…é¡µ"""
    print(f"   ğŸ” è§£æåŠ¨ä½œé¡µé¢: {url}")
    html = safe_get(url)
    data = {"exercise_url": url}
    if not html:
        print("   âš ï¸ é¡µé¢è®¿é—®å¤±è´¥")
        return data

    soup = BeautifulSoup(html, "html.parser")

    # ---------- Classification ----------
    cls = soup.find("h2", string=lambda s: s and "Classification" in s)
    if cls:
        table = cls.find_next("table")
        if table:
            for tr in table.find_all("tr"):
                tds = tr.find_all("td")
                if len(tds) == 2:
                    key = tds[0].get_text(strip=True).replace(":", "")
                    val = tds[1].get_text(" ", strip=True)
                    data[key] = val

    # ---------- Instructions ----------
    instr = soup.find("h2", string=lambda s: s and "Instructions" in s)
    if instr:
        ps = []
        for p in instr.find_all_next("p"):
            if p.find_previous_sibling("h2") == instr:
                ps.append(p.get_text(" ", strip=True))
            elif p.find_previous("h2") != instr:
                break
        data["Instructions"] = " ".join(ps)

    # ---------- Comments ----------
    comments = soup.find("h2", string=lambda s: s and "Comments" in s)
    if comments:
        ps = []
        for p in comments.find_all_next("p"):
            if p.find_previous_sibling("h2") == comments:
                ps.append(p.get_text(" ", strip=True))
            elif p.find_previous("h2") != comments:
                break
        data["Comments"] = " ".join(ps)

    # ---------- Muscles ----------
    muscles_section = soup.find("h2", string=lambda s: s and "Muscles" in s)
    if muscles_section:
        muscles = {}
        cur_cat = None
        for tag in muscles_section.find_all_next():
            if tag.name == "p":
                strong = tag.find("strong")
                if strong:
                    cur_cat = strong.get_text(strip=True)
                    muscles[cur_cat] = []
            elif tag.name == "ul" and cur_cat:
                muscles[cur_cat].extend(
                    [li.get_text(" ", strip=True) for li in tag.find_all("li")]
                )
            elif tag.name == "h2":
                break
        data["Muscles"] = muscles

    return data


# ====================================================
# éƒ¨ä½é¡µé¢è§£æ
# ====================================================
def parse_bodypart_page(body_part):
    """è§£ææŸä¸ªéƒ¨ä½é¡µé¢çš„æ‰€æœ‰åŠ¨ä½œ"""
    part_url = f"{BASE_URL}/Lists/ExList/{body_part}Wt"
    print(f"\nğŸ¦¾ æŠ“å–éƒ¨ä½é¡µé¢: {part_url}")
    html = safe_get(part_url)
    if not html:
        print(f"âŒ æ— æ³•è®¿é—® {body_part} é¡µé¢ï¼Œè·³è¿‡")
        return []

    soup = BeautifulSoup(html, "html.parser")
    results = []

    for h2 in soup.find_all("h2"):
        a_tag = h2.find("a", href=True)
        if not a_tag:
            continue
        muscle = a_tag.text.strip()
        if not muscle:
            continue

        # æ”¶é›†åˆ°ä¸‹ä¸€ä¸ª h2 ä¸ºæ­¢çš„æ‰€æœ‰ HTML å—
        section_tags = []
        for sibling in h2.find_all_next():
            if sibling.name == "h2":
                break
            section_tags.append(sibling)

        section_html = BeautifulSoup("".join(str(t) for t in section_tags), "html.parser")

        # éå† liï¼ˆè®­ç»ƒæ–¹å¼ï¼‰
        for li in section_html.find_all("li", recursive=False):
            training_type = li.find(text=True, recursive=False)
            if not training_type:
                continue
            training_type = training_type.strip()
            sub_ul = li.find("ul")
            if sub_ul:
                for a in sub_ul.find_all("a", href=True):
                    href = a["href"]
                    full_url = normalize_exercise_url(href)
                    exercise_name = a.text.strip()
                    results.append({
                        "body_part": body_part,
                        "muscle": muscle,
                        "training_type": training_type,
                        "exercise_name": exercise_name,
                        "exercise_url": full_url
                    })
    return results


# ====================================================
# ä¸»æµç¨‹
# ====================================================
def crawl_all_bodyparts():
    all_data = []

    for part in BODY_PARTS:
        exercises = parse_bodypart_page(part)
        print(f"âœ… {part}: æ‰¾åˆ° {len(exercises)} ä¸ªåŠ¨ä½œ")
        for ex in exercises:
            print(f"\n åŠ¨ä½œ: {ex['exercise_name']} ({ex['training_type']})")
            detail = parse_exercise_detail(ex["exercise_url"])
            combined = {**ex, **detail}
            all_data.append(combined)
            time.sleep(0.5)  # æ§åˆ¶é€Ÿç‡ï¼Œé˜²æ­¢å°ç¦

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    print(f"\n å…¨éƒ¨å®Œæˆï¼Œå…±æ”¶é›† {len(all_data)} æ¡åŠ¨ä½œè®°å½• â†’ {OUTPUT_JSON}")


# ====================================================
# æ‰§è¡Œå…¥å£
# ====================================================
if __name__ == "__main__":
    crawl_all_bodyparts()
